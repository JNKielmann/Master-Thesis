{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "from gensim.summarization.bm25 import BM25\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 1000)\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import spacy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n",
    "\n",
    "\n",
    "from bm25_retrieval import BM25Retrieval\n",
    "from evaluation import average_precision, mean_average_precision, mean_average_precision_parallel\n",
    "from preprocessing import apply_pipeline, Corpus, BasicPreprocessing, BigramPreprocessor, SpacyPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load corpus using different preprocessing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Start preprocessing pipeline \"basic\" for file ../data/kit_expert_2017_papers.csv.\n",
      "INFO:root:Loaded cached preprocessed corpus from ../data/kit_expert_2017_papers_basic\n",
      "INFO:root:Start preprocessing pipeline \"basic_bigrams\" for file ../data/kit_expert_2017_papers.csv.\n",
      "INFO:root:Loaded cached preprocessed corpus from ../data/kit_expert_2017_papers_basic_bigrams\n",
      "INFO:root:Start preprocessing pipeline \"basic_spacy_lemmatization\" for file ../data/kit_expert_2017_papers.csv.\n",
      "INFO:root:Loaded cached preprocessed corpus from ../data/kit_expert_2017_papers_basic_spacy_lemmatization\n",
      "INFO:root:Start preprocessing pipeline \"basic_spacy_lemmatization_bigrams\" for file ../data/kit_expert_2017_papers.csv.\n",
      "INFO:root:Loaded cached preprocessed corpus from ../data/kit_expert_2017_papers_basic_spacy_lemmatization_bigrams\n"
     ]
    }
   ],
   "source": [
    "base_file =  \"../data/kit_expert_2017_papers.csv\"\n",
    "\n",
    "p = [BasicPreprocessing()]\n",
    "papers_basic = Corpus(base_file, p)\n",
    "\n",
    "p = [BasicPreprocessing(), BigramPreprocessor()]\n",
    "papers_basic_bigram = Corpus(base_file, p)\n",
    "\n",
    "p = [BasicPreprocessing(), SpacyPreprocessor(lemmatization=True)]\n",
    "papers_basic_lemmatization = Corpus(base_file, p, load_from_cache=True)\n",
    "\n",
    "p = [BasicPreprocessing(), SpacyPreprocessor(lemmatization=True), BigramPreprocessor()]\n",
    "papers_basic_lemmatization_bigram = Corpus(base_file, p)\n",
    "\n",
    "# p = [BasicPreprocessing(), SpacyPreprocessor(combine_noun_chunks=True)]\n",
    "# papers_basic_nounchunk = Corpus(base_file, p)\n",
    "\n",
    "# p = [BasicPreprocessing(), SpacyPreprocessor(lemmatization=True, combine_noun_chunks=True)] \n",
    "# papers_basic_lemmatization_nounchunk = Corpus(base_file, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load keywords to use as test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/kit_expert_2017_keywords.json\", \"r\") as file:\n",
    "    keywords = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define models to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"BM25 unigrams\", partial(BM25Retrieval, corpus = papers_basic)),\n",
    "    (\"BM25 bigrams\", partial(BM25Retrieval, corpus = papers_basic_bigram)),\n",
    "    (\"BM25 lemmatization unigrams\", partial(BM25Retrieval, corpus = papers_basic_lemmatization)),\n",
    "    (\"BM25 lemmatization bigrams\", partial(BM25Retrieval, corpus = papers_basic_lemmatization_bigram)),\n",
    "#     (\"BM25 nounchunk\", partial(BM25Retrieval, corpus = papers_basic_nounchunk)),\n",
    "#     (\"BM25 lemmatization nounchunk\", partial(BM25Retrieval, corpus = papers_basic_lemmatization_nounchunk)),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_keywords = [k for k in keywords if k[\"level\"]<=1]\n",
    "specific_keywords = [k for k in keywords if k[\"level\"]>=2]\n",
    "test_sets = [(\"general keywords\", general_keywords), (\"specific_keywords\", specific_keywords)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mAP scores for models:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>general keywords</th>\n",
       "      <th>specific_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BM25 unigrams</th>\n",
       "      <td>0.056652</td>\n",
       "      <td>0.514452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BM25 bigrams</th>\n",
       "      <td>0.038118</td>\n",
       "      <td>0.456968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BM25 lemmatization unigrams</th>\n",
       "      <td>0.058848</td>\n",
       "      <td>0.512645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BM25 lemmatization bigrams</th>\n",
       "      <td>0.040045</td>\n",
       "      <td>0.440767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             general keywords  specific_keywords\n",
       "BM25 unigrams                        0.056652           0.514452\n",
       "BM25 bigrams                         0.038118           0.456968\n",
       "BM25 lemmatization unigrams          0.058848           0.512645\n",
       "BM25 lemmatization bigrams           0.040045           0.440767"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "for model_name, model_factory in tqdm(models):\n",
    "    model_instance = model_factory()\n",
    "    results[model_name] = {}\n",
    "    for test_set_name, test_set in test_sets:\n",
    "        data = [{\n",
    "            \"query\": keyword_info[\"keyword\"],\n",
    "            \"documents\": keyword_info[\"paper_ids\"]\n",
    "        } for keyword_info in test_set[:1000]]\n",
    "        mAP = mean_average_precision(model_instance, data)\n",
    "        results[model_name][test_set_name] = mAP\n",
    "print(\"mAP scores for models:\")\n",
    "pd.DataFrame.from_dict(results, orient=\"index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
