
@book{alvarezReviewWordEmbedding2017,
  title = {A Review of Word Embedding and Document Similarity Algorithms Applied to Academic Text},
  author = {Alvarez, Jon Ezeiza},
  date = {2017},
  publisher = {{University OF Freiburg}}
}

@article{aroraSimpleToughtobeatBaseline2016,
  title = {A Simple but Tough-to-Beat Baseline for Sentence Embeddings},
  author = {Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
  date = {2016}
}

@inproceedings{baileyOverviewTREC20072007,
  title = {Overview of the {{TREC}} 2007 {{Enterprise Track}}.},
  booktitle = {{{TREC}}},
  author = {Bailey, Peter and De Vries, Arjen P and Craswell, Nick and Soboroff, Ian},
  date = {2007},
  publisher = {{Citeseer}}
}

@inproceedings{balog2006formal,
  title = {Formal Models for Expert Finding in Enterprise Corpora},
  booktitle = {Proceedings of the 29th Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Balog, Krisztian and Azzopardi, Leif and De Rijke, Maarten},
  date = {2006},
  pages = {43--50},
  organization = {{ACM}}
}

@inproceedings{balog2008few,
  title = {A Few Examples Go a Long Way: Constructing Query Models from Elaborate Query Formulations},
  booktitle = {Proceedings of the 31st Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Balog, Krisztian and Weerkamp, Wouter and De Rijke, Maarten},
  date = {2008},
  pages = {371--378},
  organization = {{ACM}}
}

@book{balog2008people,
  title = {People Search in the Enterprise.},
  author = {Balog, Krisztian and others},
  date = {2008},
  publisher = {{Universiteit van Amsterdam [Host]}}
}

@article{balogExpertiseRetrieval2012,
  title = {Expertise {{Retrieval}}},
  author = {Balog, Krisztian},
  date = {2012},
  journaltitle = {Foundations and Trends® in Information Retrieval},
  shortjournal = {FNT in Information Retrieval},
  volume = {6},
  pages = {127--256},
  issn = {1554-0669, 1554-0677},
  doi = {10.1561/1500000024},
  url = {http://www.nowpublishers.com/article/Details/INR-024},
  urldate = {2019-11-11},
  file = {/Users/d062356/Zotero/storage/QRWMU4VZ/Balog - 2012 - Expertise Retrieval.pdf},
  langid = {english},
  number = {2-3}
}

@report{balogOverviewTREC20082008,
  title = {Overview of the {{TREC}} 2008 Enterprise Track},
  author = {Balog, Krisztian and Thomas, Paul and Craswell, Nick and Soboroff, Ian and Bailey, Peter and De Vries, Arjen P},
  date = {2008},
  institution = {{Amsterdam Univ (Netherlands)}}
}

@article{bojanowskiEnrichingWordVectors2017,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  date = {2017-06-19},
  url = {http://arxiv.org/abs/1607.04606},
  urldate = {2020-02-03},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character \$n\$-grams. A vector representation is associated to each character \$n\$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  archivePrefix = {arXiv},
  eprint = {1607.04606},
  eprinttype = {arxiv},
  file = {/Users/d062356/Zotero/storage/Y5HBDMLH/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf;/Users/d062356/Zotero/storage/N5PA62NP/1607.html},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{cao2005research,
  title = {Research on Expert Search at Enterprise Track of {{TREC}} 2005.},
  booktitle = {{{TREC}}},
  author = {Cao, Yunbo and Liu, Jingjing and Bao, Shenghua and Li, Hang},
  date = {2005}
}

@inproceedings{craswellOverviewTREC2005Enterprise2005,
  title = {Overview of the {{TREC}}-2005 Enterprise Track},
  author = {Craswell, Nick and de Vries, Arjen and Soboroff, Ian},
  date = {2005-01},
  options = {useprefix=true}
}

@article{daiDeeperTextUnderstanding2019,
  title = {Deeper {{Text Understanding}} for {{IR}} with {{Contextual Neural Language Modeling}}},
  author = {Dai, Zhuyun and Callan, Jamie},
  date = {2019},
  journaltitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval  - SIGIR'19},
  pages = {985--988},
  doi = {10.1145/3331184.3331303},
  url = {http://arxiv.org/abs/1905.09217},
  urldate = {2019-11-04},
  abstract = {Neural networks provide new possibilities to automatically learn complex language patterns and query-document relations. Neural IR models have achieved promising results in learning query-document relevance patterns, but few explorations have been done on understanding the text content of a query or a document. This paper studies leveraging a recently-proposed contextual neural language model, BERT, to provide deeper text understanding for IR. Experimental results demonstrate that the contextual text representations from BERT are more effective than traditional word embeddings. Compared to bag-of-words retrieval models, the contextual language model can better leverage language structures, bringing large improvements on queries written in natural languages. Combining the text understanding ability with search knowledge leads to an enhanced pre-trained BERT model that can benefit related search tasks where training data are limited.},
  archivePrefix = {arXiv},
  eprint = {1905.09217},
  eprinttype = {arxiv},
  file = {/Users/d062356/Zotero/storage/U5GPF3US/Dai and Callan - 2019 - Deeper Text Understanding for IR with Contextual N.pdf;/Users/d062356/Zotero/storage/6YTZTM5B/1905.html},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval}
}

@inproceedings{dengFormalModelsExpert2008,
  title = {Formal Models for Expert Finding on Dblp Bibliography Data},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Deng, Hongbo and King, Irwin and Lyu, Michael R},
  date = {2008},
  pages = {163--172},
  publisher = {{IEEE}}
}

@inproceedings{dengModelingExploitingHeterogeneous2012,
  title = {Modeling and Exploiting Heterogeneous Bibliographic Networks for Expertise Ranking},
  booktitle = {Proceedings of the 12th {{ACM}}/{{IEEE}}-{{CS}} Joint Conference on {{Digital Libraries}} - {{JCDL}} '12},
  author = {Deng, Hongbo and Han, Jiawei and Lyu, Michael R. and King, Irwin},
  date = {2012},
  pages = {71},
  publisher = {{ACM Press}},
  location = {{Washington, DC, USA}},
  doi = {10.1145/2232817.2232833},
  url = {http://dl.acm.org/citation.cfm?doid=2232817.2232833},
  urldate = {2019-11-11},
  eventtitle = {The 12th {{ACM}}/{{IEEE}}-{{CS}} Joint Conference},
  isbn = {978-1-4503-1154-0},
  langid = {english}
}

@article{ethayarajhUnderstandingLinearWord2019,
  title = {Towards {{Understanding Linear Word Analogies}}},
  author = {Ethayarajh, Kawin and Duvenaud, David and Hirst, Graeme},
  date = {2019-08-12},
  url = {http://arxiv.org/abs/1810.04882},
  urldate = {2020-02-03},
  abstract = {A surprising property of word vectors is that word analogies can often be solved with vector arithmetic. However, it is unclear why arithmetic operators correspond to non-linear embedding models such as skip-gram with negative sampling (SGNS). We provide a formal explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution. Our theory has several implications. Past work has conjectured that linear substructures exist in vector spaces because relations can be represented as ratios; we prove that this holds for SGNS. We provide novel justification for the addition of SGNS word vectors by showing that it automatically down-weights the more frequent word, as weighting schemes do ad hoc. Lastly, we offer an information theoretic interpretation of Euclidean distance in vector spaces, justifying its use in capturing word dissimilarity.},
  archivePrefix = {arXiv},
  eprint = {1810.04882},
  eprinttype = {arxiv},
  file = {/Users/d062356/Zotero/storage/J3VPGC4A/Ethayarajh et al. - 2019 - Towards Understanding Linear Word Analogies.pdf;/Users/d062356/Zotero/storage/WY6ZZTR6/1810.html},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@inproceedings{fang2006language,
  title = {Language Models for Expert {{Finding}}–{{UIUC TREC}} 2006 Enterprise Track Experiments.},
  booktitle = {{{TREC}}},
  author = {Fang, Hui and Zhou, Lixin and Zhai, ChengXiang},
  date = {2006}
}

@incollection{fangProbabilisticModelsExpert2007,
  title = {Probabilistic {{Models}} for {{Expert Finding}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Fang, Hui and Zhai, ChengXiang},
  editor = {Amati, Giambattista and Carpineto, Claudio and Romano, Giovanni},
  date = {2007},
  volume = {4425},
  pages = {418--430},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-71496-5_38},
  url = {http://link.springer.com/10.1007/978-3-540-71496-5_38},
  urldate = {2019-11-11},
  isbn = {978-3-540-71494-1 978-3-540-71496-5},
  langid = {english}
}

@inproceedings{foxCombinationMultipleSearches1993,
  title = {Combination of Multiple Searches},
  booktitle = {{{TREC}}},
  author = {Fox, Edward A. and Shaw, Joseph A.},
  date = {1993}
}

@inproceedings{gippSciensteinResearchPaper2009,
  title = {Scienstein: {{A}} Research Paper Recommender System},
  booktitle = {Proceedings of the International Conference on Emerging Trends in Computing (Icetic’09)},
  author = {Gipp, Bela and Beel, Jöran and Hentschel, Christian},
  date = {2009},
  pages = {309--315}
}

@article{guiExpertFindingHeterogeneous2018,
  title = {Expert {{Finding}} in {{Heterogeneous Bibliographic Networks}} with {{Locally}}-Trained {{Embeddings}}},
  author = {Gui, Huan and Zhu, Qi and Liu, Liyuan and Zhang, Aston and Han, Jiawei},
  date = {2018-03-08},
  url = {http://arxiv.org/abs/1803.03370},
  urldate = {2019-11-11},
  abstract = {Expert finding is an important task in both industry and academia. It is challenging to rank candidates with appropriate expertise for various queries. In addition, different types of objects interact with one another, which naturally forms heterogeneous information networks. We study the task of expert finding in heterogeneous bibliographical networks based on two aspects: textual content analysis and authority ranking. Regarding the textual content analysis, we propose a new method for query expansion via locally-trained embedding learning with concept hierarchy as guidance, which is particularly tailored for specific queries with narrow semantic meanings. Compared with global embedding learning, locally-trained embedding learning projects the terms into a latent semantic space constrained on relevant topics, therefore it preserves more precise and subtle information for specific queries. Considering the candidate ranking, the heterogeneous information network structure, while being largely ignored in the previous studies of expert finding, provides additional information. Specifically, different types of interactions among objects play different roles. We propose a ranking algorithm to estimate the authority of objects in the network, treating each strongly-typed edge type individually. To demonstrate the effectiveness of the proposed framework, we apply the proposed method to a large-scale bibliographical dataset with over two million entries and one million researcher candidates. The experiment results show that the proposed framework outperforms existing methods for both general and specific queries.},
  archivePrefix = {arXiv},
  eprint = {1803.03370},
  eprinttype = {arxiv},
  file = {/Users/d062356/Zotero/storage/7K9AFVL4/Gui et al. - 2018 - Expert Finding in Heterogeneous Bibliographic Netw.pdf;/Users/d062356/Zotero/storage/8UEVYKLH/1803.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Social and Information Networks},
  primaryClass = {cs}
}

@article{guptaBetterWordEmbeddings2019,
  title = {Better {{Word Embeddings}} by {{Disentangling Contextual}} N-{{Gram Information}}},
  author = {Gupta, Prakhar and Pagliardini, Matteo and Jaggi, Martin},
  date = {2019-04-10},
  url = {http://arxiv.org/abs/1904.05033},
  urldate = {2020-02-03},
  abstract = {Pre-trained word vectors are ubiquitous in Natural Language Processing applications. In this paper, we show how training word embeddings jointly with bigram and even trigram embeddings, results in improved unigram embeddings. We claim that training word embeddings along with higher n-gram embeddings helps in the removal of the contextual information from the unigrams, resulting in better stand-alone word embeddings. We empirically show the validity of our hypothesis by outperforming other competing word representation models by a significant margin on a wide variety of tasks. We make our models publicly available.},
  archivePrefix = {arXiv},
  eprint = {1904.05033},
  eprinttype = {arxiv},
  file = {/Users/d062356/Zotero/storage/I2AYVP7L/Gupta et al. - 2019 - Better Word Embeddings by Disentangling Contextual.pdf;/Users/d062356/Zotero/storage/KBFKFCT3/1904.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{harrisDistributionalStructure1954,
  title = {Distributional {{Structure}}},
  author = {Harris, Zellig S.},
  date = {1954-08},
  journaltitle = {\emph{WORD}},
  shortjournal = {\emph{WORD}},
  volume = {10},
  pages = {146--162},
  issn = {0043-7956, 2373-5112},
  doi = {10.1080/00437956.1954.11659520},
  url = {http://www.tandfonline.com/doi/full/10.1080/00437956.1954.11659520},
  urldate = {2020-02-03},
  langid = {english},
  number = {2-3}
}

@inproceedings{hashemiExpertiseRetrievalBibliographic2013,
  title = {Expertise Retrieval in Bibliographic Network: A Topic Dominance Learning Approach},
  booktitle = {Proceedings of the 22nd {{ACM}} International Conference on {{Conference}} on Information \& Knowledge Management},
  author = {Hashemi, Seyyed Hadi and Neshati, Mahmood and Beigy, Hamid},
  date = {2013},
  pages = {1117--1126},
  publisher = {{ACM}}
}

@article{heckExpertRecommendationKnowledge2011,
  title = {Expert Recommendation for Knowledge Management in Academia},
  author = {Heck, Tamara and Hanraths, Oliver and Stock, Wolfgang G},
  date = {2011},
  journaltitle = {Proceedings of the American Society for Information Science and Technology},
  volume = {48},
  pages = {1--4},
  number = {1}
}

@incollection{hiemstraLanguageModels2009,
  title = {Language Models},
  booktitle = {Encyclopedia of Database Systems},
  author = {Hiemstra, Djoerd},
  editor = {LIU, LING and ÖZSU, M. TAMER},
  date = {2009},
  pages = {1591--1594},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-0-387-39940-9₉23},
  url = {https://doi.org/10.1007/978-0-387-39940-9₉23},
  isbn = {978-0-387-39940-9}
}

@inproceedings{hiemstraTwentyOneTREC7Adhoc1999,
  title = {Twenty-One at TREC-7: ad-hoc and cross-language track},
  booktitle = {Proceedings of the seventh Text Retrieval Conference (TREC)},
  author = {Hiemstra, Djoerd and Kraaij, Wessel},
  editor = {Voorhees, E.M and Harman, D.K.},
  date = {1999},
  pages = {227--238},
  publisher = {{National Institute of Standards and Technology}},
  location = {{United States}},
  abstract = {This paper describes the official runs of the Twenty-One group for TREC-7. The Twenty-One group participated in the ad-hoc and the cross-language track and made the following accomplishments: We developed a new weighting algorithm, which outperforms the popular Cornell version of BM25 on the ad-hoc collection. For the CLIR task we developed a fuzzy matching algorithm to recover from missing translations and spelling variants of proper names. Also for CLIR we investigated translation strategies that make extensive use of information from our dictionaries by identifying preferred translations, main translations and synonym translations, by defining weights of possible translations and by experimenting with probabilistic boolean matching strategies.},
  keywords = {CR-H.3.3,EWI-9421,IR-66980,METIS-119693},
  langid = {Undefined},
  series = {NIST Special Publications}
}

@book{hiemstraUsingLanguageModels2001,
  title = {Using Language Models for Information Retrieval},
  author = {Hiemstra, Djoerd},
  date = {2001},
  publisher = {{Univ. Twente}}
}

@inproceedings{hossain2018rematch,
  title = {{{REMatch}}: Research Expert Matching System},
  booktitle = {2018 International Symposium on Big Data Visual and Immersive Analytics ({{BDVA}})},
  author = {Hossain, Md Iqbal and Kobourov, Stephen and Purchase, Helen and Surdeanu, Mihai},
  date = {2018},
  pages = {1--10},
  organization = {{IEEE}}
}

@incollection{jacksonSurveyUseCases2011,
  title = {Survey of {{Use Cases}} for {{Mobile Augmented Reality Browsers}}},
  booktitle = {Handbook of {{Augmented Reality}}},
  author = {Jackson, Tia and Angermann, Frank and Meier, Peter},
  editor = {Furht, Borko},
  date = {2011},
  pages = {409--431},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4614-0064-6_19},
  url = {http://link.springer.com/10.1007/978-1-4614-0064-6_19},
  urldate = {2019-11-04},
  isbn = {978-1-4614-0063-9 978-1-4614-0064-6},
  langid = {english}
}

@article{joulinBagTricksEfficient2016,
  title = {Bag of {{Tricks}} for {{Efficient Text Classification}}},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  date = {2016-08-09},
  url = {http://arxiv.org/abs/1607.01759},
  urldate = {2020-02-17},
  abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore\textasciitilde{}CPU, and classify half a million sentences among\textasciitilde{}312K classes in less than a minute.},
  archivePrefix = {arXiv},
  eprint = {1607.01759},
  eprinttype = {arxiv},
  file = {/Users/d062356/Zotero/storage/HU4QWU5D/Joulin et al. - 2016 - Bag of Tricks for Efficient Text Classification.pdf;/Users/d062356/Zotero/storage/LZNM2PQ4/1607.html},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@inproceedings{liu2005finding,
  title = {Finding Experts in Community-Based Question-Answering Services},
  booktitle = {Proceedings of the 14th {{ACM}} International Conference on {{Information}} and Knowledge Management},
  author = {Liu, Xiaoyong and Croft, W Bruce and Koll, Matthew},
  date = {2005},
  pages = {315--316},
  organization = {{ACM}}
}

@article{liuHowChooseAppropriate2015,
  title = {How to Choose Appropriate Experts for Peer Review: {{An}} Intelligent Recommendation Method in a Big Data Context},
  author = {Liu, Duanduan and Xu, Wei and Du, Wei and Wang, Fuyin},
  date = {2015},
  journaltitle = {Data Science Journal},
  volume = {14}
}

@inproceedings{luiLangidPyOfftheshelf2012,
  title = {Langid.Py: {{An}} off-the-Shelf Language Identification Tool},
  booktitle = {Proceedings of the {{ACL}} 2012 System Demonstrations},
  author = {Lui, Marco and Baldwin, Timothy},
  date = {2012-07},
  pages = {25--30},
  publisher = {{Association for Computational Linguistics}},
  location = {{Jeju Island, Korea}},
  url = {https://www.aclweb.org/anthology/P12-3005}
}

@article{macavaneyCEDRContextualizedEmbeddings2019,
  title = {{{CEDR}}: {{Contextualized Embeddings}} for {{Document Ranking}}},
  shorttitle = {{{CEDR}}},
  author = {MacAvaney, Sean and Yates, Andrew and Cohan, Arman and Goharian, Nazli},
  date = {2019},
  journaltitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval  - SIGIR'19},
  pages = {1101--1104},
  doi = {10.1145/3331184.3331317},
  url = {http://arxiv.org/abs/1904.07094},
  urldate = {2019-11-04},
  abstract = {Although considerable attention has been given to neural ranking architectures recently, far less attention has been paid to the term representations that are used as input to these models. In this work, we investigate how two pretrained contextualized language models (ELMo and BERT) can be utilized for ad-hoc document ranking. Through experiments on TREC benchmarks, we find that several existing neural ranking architectures can benefit from the additional context provided by contextualized language models. Furthermore, we propose a joint approach that incorporates BERT's classification vector into existing neural models and show that it outperforms state-of-the-art ad-hoc ranking baselines. We call this joint approach CEDR (Contextualized Embeddings for Document Ranking). We also address practical challenges in using these models for ranking, including the maximum input length imposed by BERT and runtime performance impacts of contextualized language models.},
  archivePrefix = {arXiv},
  eprint = {1904.07094},
  eprinttype = {arxiv},
  file = {/Users/d062356/Zotero/storage/LUEHJKST/MacAvaney et al. - 2019 - CEDR Contextualized Embeddings for Document Ranki.pdf;/Users/d062356/Zotero/storage/UJ8E4NNG/1904.html},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval}
}

@inproceedings{macdonald2005university,
  title = {University of Glasgow at {{TREC}} 2005: Experiments in Terabyte and Enterprise Tracks with Terrier.},
  booktitle = {{{TREC}}},
  author = {Macdonald, Craig and He, Ben and Plachouras, Vassilis and Ounis, Iadh},
  date = {2005}
}

@inproceedings{macdonald2007expertise,
  title = {Expertise Drift and Query Expansion in Expert Search},
  booktitle = {Proceedings of the Sixteenth {{ACM}} Conference on {{Conference}} on Information and Knowledge Management},
  author = {Macdonald, Craig and Ounis, Iadh},
  date = {2007},
  pages = {341--350},
  organization = {{ACM}}
}

@inproceedings{macdonaldVotingCandidatesAdapting2006,
  title = {Voting for Candidates: {{Adapting}} Data Fusion Techniques for an Expert Search Task},
  booktitle = {Proceedings of the 15th {{ACM}} International Conference on Information and Knowledge Management},
  author = {Macdonald, Craig and Ounis, Iadh},
  date = {2006},
  pages = {387--396},
  publisher = {{Association for Computing Machinery}},
  location = {{Arlington, Virginia, USA}},
  doi = {10.1145/1183614.1183671},
  url = {https://doi.org/10.1145/1183614.1183671},
  isbn = {1-59593-433-2},
  keywords = {data fusion,expert finding,expert search,expertise modelling,information retrieval,ranking,voting},
  numpages = {10},
  series = {{{CIKM}} ’06}
}

@book{manningIntroductionInformationRetrieval2008,
  title = {Introduction to Information Retrieval},
  author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
  date = {2008},
  publisher = {{Cambridge University Press}},
  location = {{New York}},
  isbn = {978-0-521-86571-5},
  keywords = {Document clustering,Information retrieval,Semantic Web,Text processing (Computer science)},
  note = {OCLC: ocn190786122},
  pagetotal = {482}
}

@incollection{melucciVectorspaceModel2009,
  title = {Vector-Space Model},
  booktitle = {Encyclopedia of Database Systems},
  author = {Melucci, Massimo},
  editor = {LIU, LING and ÖZSU, M. TAMER},
  date = {2009},
  pages = {3259--3263},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-0-387-39940-9₉18},
  url = {https://doi.org/10.1007/978-0-387-39940-9₉18},
  isbn = {978-0-387-39940-9}
}

@article{mikolovDistributedRepresentationsWords2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013-10-16},
  url = {http://arxiv.org/abs/1310.4546},
  urldate = {2020-01-22},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  archivePrefix = {arXiv},
  eprint = {1310.4546},
  eprinttype = {arxiv},
  file = {/Users/d062356/Zotero/storage/BU398W8K/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf;/Users/d062356/Zotero/storage/TICEN5AX/1310.html},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{mikolovEfficientEstimationWord2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013-09-06},
  url = {http://arxiv.org/abs/1301.3781},
  urldate = {2020-01-22},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archivePrefix = {arXiv},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  file = {/Users/d062356/Zotero/storage/YSNSRFJ2/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/Users/d062356/Zotero/storage/NCMVZGRS/1301.html},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@inproceedings{millerHiddenMarkovModel1999,
  title = {A Hidden {{Markov}} Model Information Retrieval System},
  booktitle = {{{SIGIR}}},
  author = {Miller, David RH and Leek, Tim and Schwartz, Richard M},
  date = {1999},
  volume = {99},
  pages = {214--221}
}

@inproceedings{mimnoExpertiseModelingMatching2007,
  title = {Expertise Modeling for Matching Papers with Reviewers},
  booktitle = {Proceedings of the 13th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Mimno, David and McCallum, Andrew},
  date = {2007},
  pages = {500--509},
  publisher = {{ACM}}
}

@article{mitraDualEmbeddingSpace2016,
  title = {A {{Dual Embedding Space Model}} for {{Document Ranking}}},
  author = {Mitra, Bhaskar and Nalisnick, Eric and Craswell, Nick and Caruana, Rich},
  date = {2016-02-02},
  url = {http://arxiv.org/abs/1602.01137},
  urldate = {2020-01-22},
  abstract = {A fundamental goal of search engines is to identify, given a query, documents that have relevant text. This is intrinsically difficult because the query and the document may use different vocabulary, or the document may contain query words without being relevant. We investigate neural word embeddings as a source of evidence in document ranking. We train a word2vec embedding model on a large unlabelled query corpus, but in contrast to how the model is commonly used, we retain both the input and the output projections, allowing us to leverage both the embedding spaces to derive richer distributional relationships. During ranking we map the query words into the input space and the document words into the output space, and compute a query-document relevance score by aggregating the cosine similarities across all the query-document word pairs. We postulate that the proposed Dual Embedding Space Model (DESM) captures evidence on whether a document is about a query term in addition to what is modelled by traditional term-frequency based approaches. Our experiments show that the DESM can re-rank top documents returned by a commercial Web search engine, like Bing, better than a term-matching based signal like TF-IDF. However, when ranking a larger set of candidate documents, we find the embeddings-based approach is prone to false positives, retrieving documents that are only loosely related to the query. We demonstrate that this problem can be solved effectively by ranking based on a linear mixture of the DESM and the word counting features.},
  archivePrefix = {arXiv},
  eprint = {1602.01137},
  eprinttype = {arxiv},
  file = {/Users/d062356/Zotero/storage/VESAXB3B/Mitra et al. - 2016 - A Dual Embedding Space Model for Document Ranking.pdf;/Users/d062356/Zotero/storage/RTXRZUMV/1602.html},
  keywords = {Computer Science - Information Retrieval},
  primaryClass = {cs}
}

@article{mitraLearningMatchUsing2016,
  title = {Learning to {{Match Using Local}} and {{Distributed Representations}} of {{Text}} for {{Web Search}}},
  author = {Mitra, Bhaskar and Diaz, Fernando and Craswell, Nick},
  date = {2016-10-25},
  url = {http://arxiv.org/abs/1610.08136},
  urldate = {2019-10-21},
  abstract = {Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favorable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or `duet' performs significantly better than either neural network individually on a Web page ranking task, and also significantly outperforms traditional baselines and other recently proposed models based on neural networks.},
  archivePrefix = {arXiv},
  eprint = {1610.08136},
  eprinttype = {arxiv},
  file = {/Users/d062356/Zotero/storage/8I65DDKA/Mitra et al. - 2016 - Learning to Match Using Local and Distributed Repr.pdf;/Users/d062356/Zotero/storage/XAKV85A4/1610.html},
  keywords = {Computer Science - Information Retrieval},
  primaryClass = {cs}
}

@article{mitraNeuralModelsInformation2017,
  title = {Neural {{Models}} for {{Information Retrieval}}},
  author = {Mitra, Bhaskar and Craswell, Nick},
  date = {2017-05-03},
  url = {http://arxiv.org/abs/1705.01509},
  urldate = {2019-10-21},
  abstract = {Neural ranking models for information retrieval (IR) use shallow or deep neural networks to rank search results in response to a query. Traditional learning to rank models employ machine learning techniques over hand-crafted IR features. By contrast, neural models learn representations of language from raw text that can bridge the gap between query and document vocabulary. Unlike classical IR models, these new machine learning based approaches are data-hungry, requiring large scale training data before they can be deployed. This tutorial introduces basic concepts and intuitions behind neural IR models, and places them in the context of traditional retrieval models. We begin by introducing fundamental concepts of IR and different neural and non-neural approaches to learning vector representations of text. We then review shallow neural IR methods that employ pre-trained neural term embeddings without learning the IR task end-to-end. We introduce deep neural networks next, discussing popular deep architectures. Finally, we review the current DNN models for information retrieval. We conclude with a discussion on potential future directions for neural IR.},
  archivePrefix = {arXiv},
  eprint = {1705.01509},
  eprinttype = {arxiv},
  file = {/Users/d062356/Zotero/storage/QJMMB7CH/Mitra and Craswell - 2017 - Neural Models for Information Retrieval.pdf;/Users/d062356/Zotero/storage/N6J8ULKN/1705.html},
  keywords = {Computer Science - Information Retrieval},
  primaryClass = {cs}
}

@article{moreira2015learning,
  title = {Learning to Rank Academic Experts in the {{DBLP}} Dataset},
  author = {Moreira, Catarina and Calado, Pável and Martins, Bruno},
  date = {2015},
  journaltitle = {Expert Systems},
  volume = {32},
  pages = {477--493},
  publisher = {{Wiley Online Library}},
  number = {4}
}

@article{nikzad-khasmakhiStateoftheartExpertRecommendation2019,
  title = {The State-of-the-Art in Expert Recommendation Systems},
  author = {Nikzad–Khasmakhi, N and Balafar, MA and Feizi–Derakhshi, M Reza},
  date = {2019},
  journaltitle = {Engineering Applications of Artificial Intelligence},
  volume = {82},
  pages = {126--147}
}

@inproceedings{pavanSemanticbasedExpertSearch2015,
  title = {Semantic-Based {{Expert Search}} in {{Textbook Research Archives}}.},
  booktitle = {{{SDA}}@ {{TPDL}}},
  author = {Pavan, Marco and De Luca, Ernesto William},
  date = {2015},
  pages = {18--29}
}

@article{philipApplicationContentbasedApproach2014,
  title = {Application of Content-Based Approach in Research Paper Recommendation System for a Digital Library},
  author = {Philip, Simon and Shola, P and Ovye, A},
  date = {2014},
  journaltitle = {International Journal of Advanced Computer Science and Applications},
  volume = {5},
  number = {10}
}

@thesis{ponteLanguageModelingApproach1998,
  title = {A Language Modeling Approach to Information Retrieval},
  author = {Ponte, Jay Michael and Croft, W Bruce},
  date = {1998},
  institution = {{University of Massachusetts at Amherst}}
}

@inproceedings{robertsonOkapiTREC31995,
  title = {Okapi at {{TREC}}-3},
  booktitle = {Overview of the Third Text {{REtrieval}} Conference ({{TREC}}-3)},
  author = {Robertson, Stephen and Walker, S. and Jones, S. and Hancock-Beaulieu, M. M. and Gatford, M.},
  date = {1995-01},
  edition = {Overview of the Third Text REtrieval Conference (TREC–3)},
  pages = {109--126},
  publisher = {{Gaithersburg, MD: NIST}},
  url = {https://www.microsoft.com/en-us/research/publication/okapi-at-trec-3/},
  abstract = {The Okapi software used for TREC-3 was similar to that used in previous TRECs, comprising a low level basic search system and a user interface for the manual search experiments, together with data conversion and inversion utilities. There were also various scripts and programs for generating query terms, running batches of trials and performing evaluation. The main code is written in C, with additional material in awk and perl. The evaluation program is from Chris Buckley at Cornell.}
}

@article{robertsonProbabilisticRelevanceFramework2009,
  title = {The Probabilistic Relevance Framework: {{BM25}} and Beyond},
  author = {Robertson, Stephen and Zaragoza, Hugo and others},
  date = {2009},
  journaltitle = {Foundations and Trends® in Information Retrieval},
  volume = {3},
  pages = {333--389},
  publisher = {{Now Publishers, Inc.}},
  number = {4}
}

@book{saltonIntroductionModernInformation1986,
  title = {Introduction to Modern Information Retrieval},
  author = {Salton, Gerard and McGill, Michael J.},
  date = {1986},
  publisher = {{McGraw-Hill, Inc.}},
  location = {{USA}},
  isbn = {0-07-054484-0}
}

@inproceedings{serdyukovModelingMultistepRelevance2008,
  title = {Modeling Multi-Step Relevance Propagation for Expert Finding},
  booktitle = {Proceedings of the 17th {{ACM}} Conference on {{Information}} and Knowledge Management},
  author = {Serdyukov, Pavel and Rode, Henning and Hiemstra, Djoerd},
  date = {2008},
  pages = {1133--1142}
}

@inproceedings{sinhaOverviewMicrosoftAcademic2015,
  title = {An Overview of Microsoft Academic Service (Mas) and Applications},
  booktitle = {Proceedings of the 24th International Conference on World Wide Web},
  author = {Sinha, Arnab and Shen, Zhihong and Song, Yang and Ma, Hao and Eide, Darrin and Hsu, Bo-June and Wang, Kuansan},
  date = {2015},
  pages = {243--246}
}

@report{smuckerInvestigationDirichletPrior2005,
  title = {An Investigation of Dirichlet Prior Smoothing’s Performance Advantage},
  author = {Smucker, Mark D and Allan, James},
  date = {2005},
  institution = {{Citeseer}}
}

@inproceedings{soboroffOverviewTREC20062006,
  title = {Overview of the {{TREC}} 2006 Enterprise Track.},
  booktitle = {Trec},
  author = {Soboroff, Ian and de Vries, Arjen P and Craswell, Nick},
  date = {2006},
  volume = {6},
  pages = {1--20},
  options = {useprefix=true}
}

@inproceedings{sorg2011finding,
  title = {Finding the Right Expert: {{Discriminative}} Models for Expert Retrieval},
  booktitle = {Proceedings of the International Conference on Knowledge Discovery and Information Retrieval ({{KDIR2011}})},
  author = {Sorg, Philipp and Cimiano, Philipp},
  date = {2011}
}

@article{sparckjonesStatisticalInterpretationTerm1972,
  title = {A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
  author = {Sparck Jones, Karen},
  date = {1972},
  journaltitle = {Journal of documentation},
  volume = {28},
  pages = {11--21},
  publisher = {{MCB UP Ltd}},
  number = {1}
}

@inproceedings{torresEnhancingDigitalLibraries2004,
  title = {Enhancing Digital Libraries with {{TechLens}}+},
  booktitle = {Proceedings of the 2004 Joint {{ACM}}/{{IEEE}} Conference on {{Digital}} Libraries  - {{JCDL}} '04},
  author = {Torres, Roberto and McNee, Sean M. and Abel, Mara and Konstan, Joseph A. and Riedl, John},
  date = {2004},
  pages = {228},
  publisher = {{ACM Press}},
  location = {{Tuscon, AZ, USA}},
  doi = {10.1145/996350.996402},
  url = {http://portal.acm.org/citation.cfm?doid=996350.996402},
  urldate = {2019-11-08},
  eventtitle = {The 2004 Joint {{ACM}}/{{IEEE}} Conference},
  isbn = {978-1-58113-832-0},
  langid = {english}
}

@article{vangyselNeuralVectorSpaces2018,
  title = {Neural {{Vector Spaces}} for {{Unsupervised Information Retrieval}}},
  author = {Van Gysel, Christophe and de Rijke, Maarten and Kanoulas, Evangelos},
  date = {2018-06-26},
  journaltitle = {ACM Transactions on Information Systems},
  shortjournal = {ACM Trans. Inf. Syst.},
  volume = {36},
  pages = {1--25},
  issn = {10468188},
  doi = {10.1145/3196826},
  url = {http://arxiv.org/abs/1708.02702},
  urldate = {2019-11-04},
  abstract = {We propose the Neural Vector Space Model (NVSM), a method that learns representations of documents in an unsupervised manner for news article retrieval. In the NVSM paradigm, we learn low-dimensional representations of words and documents from scratch using gradient descent and rank documents according to their similarity with query representations that are composed from word representations. We show that NVSM performs better at document ranking than existing latent semantic vector space methods. The addition of NVSM to a mixture of lexical language models and a state-of-the-art baseline vector space model yields a statistically significant increase in retrieval effectiveness. Consequently, NVSM adds a complementary relevance signal. Next to semantic matching, we find that NVSM performs well in cases where lexical matching is needed. NVSM learns a notion of term specificity directly from the document collection without feature engineering. We also show that NVSM learns regularities related to Luhn significance. Finally, we give advice on how to deploy NVSM in situations where model selection (e.g., cross-validation) is infeasible. We find that an unsupervised ensemble of multiple models trained with different hyperparameter values performs better than a single cross-validated model. Therefore, NVSM can safely be used for ranking documents without supervised relevance judgments.},
  archivePrefix = {arXiv},
  eprint = {1708.02702},
  eprinttype = {arxiv},
  file = {/Users/d062356/Zotero/storage/3I8ED6WI/Van Gysel et al. - 2018 - Neural Vector Spaces for Unsupervised Information .pdf;/Users/d062356/Zotero/storage/7VAADYQ6/1708.html},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  number = {4},
  options = {useprefix=true}
}

@article{vangyselUnsupervisedEfficientSemantic2016,
  title = {Unsupervised, {{Efficient}} and {{Semantic Expertise Retrieval}}},
  author = {Van Gysel, Christophe and de Rijke, Maarten and Worring, Marcel},
  date = {2016},
  journaltitle = {Proceedings of the 25th International Conference on World Wide Web - WWW '16},
  pages = {1069--1079},
  doi = {10.1145/2872427.2882974},
  url = {http://arxiv.org/abs/1608.06651},
  urldate = {2019-11-11},
  abstract = {We introduce an unsupervised discriminative model for the task of retrieving experts in online document collections. We exclusively employ textual evidence and avoid explicit feature engineering by learning distributed word representations in an unsupervised way. We compare our model to state-of-the-art unsupervised statistical vector space and probabilistic generative approaches. Our proposed log-linear model achieves the retrieval performance levels of state-of-the-art document-centric methods with the low inference cost of so-called profile-centric approaches. It yields a statistically significant improved ranking over vector space and generative models in most cases, matching the performance of supervised methods on various benchmarks. That is, by using solely text we can do as well as methods that work with external evidence and/or relevance feedback. A contrastive analysis of rankings produced by discriminative and generative approaches shows that they have complementary strengths due to the ability of the unsupervised discriminative model to perform semantic matching.},
  archivePrefix = {arXiv},
  eprint = {1608.06651},
  eprinttype = {arxiv},
  file = {/Users/d062356/Zotero/storage/N4RWVHIW/Van Gysel et al. - 2016 - Unsupervised, Efficient and Semantic Expertise Ret.pdf;/Users/d062356/Zotero/storage/TP89B66Y/1608.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  options = {useprefix=true}
}

@article{wangSurveyExpertRecommendation2018,
  title = {A {{Survey}} on {{Expert Recommendation}} in {{Community Question Answering}}},
  author = {Wang, Xianzhi and Huang, Chaoran and Yao, Lina and Benatallah, Boualem and Dong, Manqing},
  date = {2018-07-15},
  url = {http://arxiv.org/abs/1807.05540},
  urldate = {2019-10-21},
  abstract = {Community question answering (CQA) represents the type of Web applications where people can exchange knowledge via asking and answering questions. One significant challenge of most real-world CQA systems is the lack of effective matching between questions and the potential good answerers, which adversely affects the efficient knowledge acquisition and circulation. On the one hand, a requester might experience many low-quality answers without receiving a quality response in a brief time, on the other hand, an answerer might face numerous new questions without being able to identify their questions of interest quickly. Under this situation, expert recommendation emerges as a promising technique to address the above issues. Instead of passively waiting for users to browse and find their questions of interest, an expert recommendation method raises the attention of users to the appropriate questions actively and promptly. The past few years have witnessed considerable efforts that address the expert recommendation problem from different perspectives. These methods all have their issues that need to be resolved before the advantages of expert recommendation can be fully embraced. In this survey, we first present an overview of the research efforts and state-of-the-art techniques for the expert recommendation in CQA. We next summarize and compare the existing methods concerning their advantages and shortcomings, followed by discussing the open issues and future research directions.},
  archivePrefix = {arXiv},
  eprint = {1807.05540},
  eprinttype = {arxiv},
  file = {/Users/d062356/Zotero/storage/7QQYZKSF/Wang et al. - 2018 - A Survey on Expert Recommendation in Community Que.pdf;/Users/d062356/Zotero/storage/ZGV95YC4/1807.html},
  keywords = {Computer Science - Information Retrieval,Computer Science - Social and Information Networks},
  primaryClass = {cs}
}

@inproceedings{yang2014using,
  title = {Using Google Distance for Query Expansion in Expert Finding},
  booktitle = {Ninth International Conference on Digital Information Management ({{ICDIM}} 2014)},
  author = {Yang, Kai-Hsiang and Lin, Yu-Li and Chuang, Chen-Tao},
  date = {2014},
  pages = {104--109},
  organization = {{IEEE}}
}

@inproceedings{yangExpert2boleExpertFinding2009,
  title = {Expert2bole: {{From}} Expert Finding to Bole Search},
  booktitle = {Proceedings of the {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data {{Mining}},({{KDD}}’09)},
  author = {Yang, Zi and Tang, Jie and Wang, Bo and Guo, Jingyi and Li, Juanzi and Chen, Songcan},
  date = {2009},
  pages = {1--4}
}

@article{zhaiStatisticalLanguageModels2008,
  title = {Statistical Language Models for Information Retrieval},
  author = {Zhai, ChengXiang},
  date = {2008},
  journaltitle = {Synthesis Lectures on Human Language Technologies},
  volume = {1},
  pages = {1--141},
  publisher = {{Morgan \& Claypool Publishers}},
  number = {1}
}

@inproceedings{zhaiStudySmoothingMethods2001,
  title = {A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval},
  booktitle = {Proceedings of the 24th Annual International {{ACM SIGIR}} Conference on Research and Development in Information Retrieval},
  author = {Zhai, Chengxiang and Lafferty, John},
  date = {2001},
  pages = {334--342},
  publisher = {{Association for Computing Machinery}},
  location = {{New Orleans, Louisiana, USA}},
  doi = {10.1145/383952.384019},
  url = {https://doi.org/10.1145/383952.384019},
  isbn = {1-58113-331-6},
  numpages = {9},
  series = {{{SIGIR}} ’01}
}

@inproceedings{zhaiStudySmoothingMethods2017,
  title = {A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval},
  booktitle = {{{ACM SIGIR Forum}}},
  author = {Zhai, Chengxiang and Lafferty, John},
  date = {2017},
  volume = {51},
  pages = {268--276},
  number = {2},
  organization = {{ACM}}
}

@inproceedings{zhangExpertFindingSocial2007,
  title = {Expert Finding in a Social Network},
  booktitle = {International Conference on Database Systems for Advanced Applications},
  author = {Zhang, Jing and Tang, Jie and Li, Juanzi},
  date = {2007},
  pages = {1066--1069},
  organization = {{Springer}}
}

@misc{zotero-6,
  type = {misc}
}


